{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_english(filename):\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    toReturn = []\n",
    "    for phrase in file:\n",
    "        toAdd = phrase.split()\n",
    "        toAdd.insert(0, \"*\")\n",
    "        toReturn.append(toAdd)\n",
    "    return toReturn\n",
    "\n",
    "def read_spanish(filename):\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    toReturn = []\n",
    "    toRemove = []\n",
    "    for phrase in file:\n",
    "        toReturn.append(phrase.split())\n",
    "    i = 0\n",
    "    for phrase in toReturn:\n",
    "        if len(phrase) == 0:\n",
    "            toRemove.append(i)\n",
    "        i += 1\n",
    "    return toReturn, toRemove\n",
    "\n",
    "print(\"Reading in corpus data...\\n\")\n",
    "english = read_english(\"corpus.en\")\n",
    "spanish, remove = read_spanish(\"corpus.es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing empty phrases...\\n\")\n",
    "for none in sorted(remove, reverse=True):\n",
    "    del english[none]\n",
    "    del spanish[none]\n",
    "print(\"English dataset is \" + str(len(english)))\n",
    "print(\"Spanish dataset is \" + str(len(spanish)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = []\n",
    "spanish_words = []\n",
    "english_indices={}\n",
    "spanish_indices={}\n",
    "print(\"Collecting words and indices...\\n\")\n",
    "for i in range(len(english)):\n",
    "    for eword in english[i]:\n",
    "        if eword not in english_words:\n",
    "            english_words.append(eword)\n",
    "    for sword in spanish[i]:\n",
    "        if sword not in spanish_words:\n",
    "            spanish_words.append(sword)\n",
    "    for i in range(len(english_words)):\n",
    "        english_indices[english_words[i]]=i\n",
    "    for i in range(len(spanish_words)):\n",
    "        spanish_indices[spanish_words[i]]=i    \n",
    "print(\"Spanish words are \" + str(len(spanish_words)))\n",
    "print(\"English words are \" + str(len(english_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading t numbers from IBM Model 1...\\n\")\n",
    "filename = \"t_params.txt\"\n",
    "t_params = np.loadtxt(filename, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating first generation t numbers...\\n\")\n",
    "count = {}\n",
    "base_t=[]\n",
    "parallel = zip(english, spanish)\n",
    "visited = set()\n",
    "for f, s in parallel:\n",
    "    for f_j in f:\n",
    "        for s_i in s:\n",
    "            pair = (f_j, s_i)\n",
    "            if pair not in visited:\n",
    "                visited.add(pair)\n",
    "                if not f_j in count:\n",
    "                    count[f_j] = 0\n",
    "                count[f_j]+=1\n",
    "                \n",
    "for i in english_words:\n",
    "    base_t.append(1/ count[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "print(\"Generating q parameters...\\n\")\n",
    "visited = {}\n",
    "visited_e = {}\n",
    "visited_s = {}\n",
    "for j in range(len(spanish)):\n",
    "    l=len(english[j])\n",
    "    m=len(spanish[j])\n",
    "    if (l,m) not in visited.keys():\n",
    "        visited_e[(l,m)]= {}\n",
    "        visited_s[(l,m)]= {}\n",
    "        s_x = len(set(english[j]))\n",
    "        s_y = len(set(spanish[j]))\n",
    "        visited[(l,m)] = sp.dok_matrix((s_x, s_y), dtype=np.float32)\n",
    "        for i in range(len(set(english[j]))):\n",
    "            visited_e[(l,m)][list(set(english[j]))[i]] = i\n",
    "        for i in range(len(set(spanish[j]))):\n",
    "            visited_s[(l,m)][list(set(spanish[j]))[i]] = i\n",
    "    else:\n",
    "        l_e=list(set(english[j]))\n",
    "        l_s=list(set(spanish[j]))\n",
    "        for i in range(len(l_e)):\n",
    "            assigned = visited_e[(l,m)]\n",
    "            if l_e[i] not in assigned.keys():\n",
    "                assigned[l_e[i]]=len(assigned.keys())\n",
    "        for i in range(len(l_s)):\n",
    "            if l_s[i] not in visited_s[(l,m)].keys():\n",
    "                visited_s[(l,m)][l_s[i]]= len(visited_s[(l,m)].keys())\n",
    "        x = len(visited_e[(l,m)].keys())\n",
    "        y = len(visited_s[(l,m)].keys())\n",
    "        visited[(l,m)]=sp.dok_matrix((x, y), dtype=np.float32)\n",
    "base_q=copy.deepcopy(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(i, k):\n",
    "    s_word = spanish[k][i]\n",
    "    count=[]\n",
    "    l = len(english[k]) \n",
    "    sum = 0\n",
    "    m = len(spanish[k])\n",
    "    for x in range(l):\n",
    "        e_word=english[k][x]\n",
    "        t = t_params[english_indices[e_word],spanish_indices[s_word]]\n",
    "        e = q_e[(l,m)][e_word]\n",
    "        s = q_s[(l,m)][s_word]\n",
    "        t_q= q[(l,m)][e,s]\n",
    "        if t_q==0:\n",
    "            t_q= 1/(l)          \n",
    "        if t==0:\n",
    "            t= base_t[english_indices[e_word]]\n",
    "        sum += (t*t_q)\n",
    "        count.append(t*t_q)\n",
    "    return np.array(count)/sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_f(counts_for_q, q):\n",
    "    for x in range(len(list(counts_for_q.keys()))):\n",
    "        (l, m)=list(counts_for_q.keys())[x]\n",
    "        q[(l, m)]=counts_for_q[(l, m)]/(counts_for_q[(l, m)].sum(1))\n",
    "    return q\n",
    "\n",
    "def t(counts):\n",
    "    r = counts.sum(1)\n",
    "    #print(r)\n",
    "    return counts/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating t and q parameters...\\n\")\n",
    "n = 5\n",
    "for s in range(n):\n",
    "    print(str(s) + \" out of \" + str(n) + \" iterations\")\n",
    "    counts = sp.dok_matrix((len(english_words), len(spanish_words)), dtype=np.float32)\n",
    "    c_q=copy.deepcopy(base_q)\n",
    "    for k in range(len(english)): \n",
    "        print(\"\\t\" + str(k) + \" out of \" + str(len(english)))\n",
    "        l = len(english[k])\n",
    "        m = len(spanish[k])\n",
    "        for i in range(len(spanish[k])):\n",
    "            delta = delta(i, k)\n",
    "            s_w = spanish[k][i]\n",
    "            s_i = spanish_indices[s_w]\n",
    "            for j in range(len(english[k])):\n",
    "                e_w = english[k][j]\n",
    "                e_i = english_indices[e_w]\n",
    "                counts[e_i, s_i] += delta[j]\n",
    "                x = q_e[(l,m)][e_w]\n",
    "                y = q_s[(l,m)][s_w]\n",
    "                c_q[(l, m)][x,y] += delta[j]\n",
    "    t_params=t(counts)\n",
    "    q = q_f(c_q, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dev data corpus...\\n\")\n",
    "english_file = open('dev.en', encoding='utf-8')\n",
    "spanish_file = open('dev.es', encoding='utf-8')\n",
    "e_dev = []\n",
    "s_dev = []\n",
    "for phrase in english_file:\n",
    "    e_dev.append(phrase.split())\n",
    "for phrase in spanish_file:\n",
    "    s_dev.append(phrase.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=open(\"dev.out\", \"w\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(s_dev)):\n",
    "    l=len(e_dev[k])\n",
    "    m=len(s_dev[k])\n",
    "    for spn_ind,spn_word in enumerate(s_dev[k],1):\n",
    "        if spn_word in spanish_words:\n",
    "            c_max=0\n",
    "            s_index=spanish_indices[spn_word];\n",
    "            c_index=0\n",
    "            for eng_ind,eng_word in enumerate(e_dev[k],1):\n",
    "                e_index= english_indices[eng_word] \n",
    "                t_value=t_params[e_index,s_index]\n",
    "                if (l,m) not in q.keys()\n",
    "                    q_value=1e-10\n",
    "                else:\n",
    "                    q_value=q[(l,m)][eng_ind-2, spn_ind-1]\n",
    "                if t_value==0:\n",
    "                    t_value=base_t[e_index]\n",
    "                if q_value ==0:\n",
    "                    q_value = 1/(l+1)\n",
    "                prd= (q_value)*(t_value)\n",
    "                if prd > c_max:\n",
    "                    c_max=prd\n",
    "                    c_index= eng_ind\n",
    "            outfile.write(str(k+1)+' '+str(c_index)+' '+str(spn_ind)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile.close()\n",
    "print(\"Evaluating...\\n\")\n",
    "os.system(\"python eval_alignment.py dev.key dev.out\")\n",
    "#!python eval_alignment.py 'dev.key' 'dev.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
